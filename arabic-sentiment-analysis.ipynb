{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install protobuf==3.20.3 --force-reinstall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:50.864178Z","iopub.execute_input":"2026-01-10T12:12:50.864909Z","iopub.status.idle":"2026-01-10T12:12:55.899601Z","shell.execute_reply.started":"2026-01-10T12:12:50.864874Z","shell.execute_reply":"2026-01-10T12:12:55.898781Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==3.20.3\n  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\nDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-3.20.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# if you need (uncomment if missing in your environment)\n#!pip -q install transformers datasets scikit-learn torch sentencepiece\n\nimport re\nimport numpy as np\nimport pandas as pd\nfrom dataclasses import dataclass\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\nimport torch\nfrom transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n                          TrainingArguments, Trainer)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:12:55.901296Z","iopub.execute_input":"2026-01-10T12:12:55.901583Z","iopub.status.idle":"2026-01-10T12:13:29.294088Z","shell.execute_reply.started":"2026-01-10T12:12:55.901557Z","shell.execute_reply":"2026-01-10T12:13:29.293537Z"}},"outputs":[{"name":"stderr","text":"2026-01-10 12:13:09.699370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768047189.892789      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768047189.949890      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!git clone https://github.com/hadyelsahar/large-arabic-sentiment-analysis-resouces.git ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:29.294766Z","iopub.execute_input":"2026-01-10T12:13:29.295230Z","iopub.status.idle":"2026-01-10T12:13:30.958969Z","shell.execute_reply.started":"2026-01-10T12:13:29.295201Z","shell.execute_reply":"2026-01-10T12:13:30.958234Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'large-arabic-sentiment-analysis-resouces'...\nremote: Enumerating objects: 60, done.\u001b[K\nremote: Total 60 (delta 0), reused 0 (delta 0), pack-reused 60 (from 1)\u001b[K\nReceiving objects: 100% (60/60), 19.14 MiB | 22.94 MiB/s, done.\nResolving deltas: 100% (22/22), done.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!git clone https://github.com/mohataher/arabic-stop-words.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:30.961122Z","iopub.execute_input":"2026-01-10T12:13:30.961436Z","iopub.status.idle":"2026-01-10T12:13:31.309384Z","shell.execute_reply.started":"2026-01-10T12:13:30.961413Z","shell.execute_reply":"2026-01-10T12:13:31.308665Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'arabic-stop-words'...\nremote: Enumerating objects: 48, done.\u001b[K\nremote: Counting objects: 100% (11/11), done.\u001b[K\nremote: Compressing objects: 100% (11/11), done.\u001b[K\nremote: Total 48 (delta 3), reused 2 (delta 0), pack-reused 37 (from 1)\u001b[K\nReceiving objects: 100% (48/48), 25.35 KiB | 12.67 MiB/s, done.\nResolving deltas: 100% (20/20), done.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install pyarabic\n\nimport pyarabic.araby as araby\nimport pyarabic.number as number","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:31.310304Z","iopub.execute_input":"2026-01-10T12:13:31.310607Z","iopub.status.idle":"2026-01-10T12:13:34.544256Z","shell.execute_reply.started":"2026-01-10T12:13:31.310580Z","shell.execute_reply":"2026-01-10T12:13:34.543066Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyarabic in /usr/local/lib/python3.11/dist-packages (0.6.15)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from pyarabic) (1.17.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"AAT = pd.read_csv('large-arabic-sentiment-analysis-resouces/datasets/ATT.csv')\nHTL = pd.read_csv('large-arabic-sentiment-analysis-resouces/datasets/HTL.csv')\nMOV = pd.read_csv('large-arabic-sentiment-analysis-resouces/datasets/MOV.csv')\nPROD = pd.read_csv('large-arabic-sentiment-analysis-resouces/datasets/PROD.csv')\nRES = pd.read_csv('large-arabic-sentiment-analysis-resouces/datasets/RES.csv')\nRES1 = pd.read_csv('large-arabic-sentiment-analysis-resouces/datasets/RES1.csv')\nRES2 = pd.read_csv('large-arabic-sentiment-analysis-resouces/datasets/RES2.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:34.545226Z","iopub.execute_input":"2026-01-10T12:13:34.545494Z","iopub.status.idle":"2026-01-10T12:13:35.165331Z","shell.execute_reply.started":"2026-01-10T12:13:34.545467Z","shell.execute_reply":"2026-01-10T12:13:35.164729Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!ls arabic-stop-words\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:35.165961Z","iopub.execute_input":"2026-01-10T12:13:35.166180Z","iopub.status.idle":"2026-01-10T12:13:35.309001Z","shell.execute_reply.started":"2026-01-10T12:13:35.166154Z","shell.execute_reply":"2026-01-10T12:13:35.308314Z"}},"outputs":[{"name":"stdout","text":"fahrasa.png  LICENSE  list.txt\tREADME.md\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"stopwords = pd.read_csv(\"arabic-stop-words/list.txt\", header=None, names=[\"word\"])\nstopwords.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:35.310028Z","iopub.execute_input":"2026-01-10T12:13:35.310319Z","iopub.status.idle":"2026-01-10T12:13:35.335821Z","shell.execute_reply.started":"2026-01-10T12:13:35.310273Z","shell.execute_reply":"2026-01-10T12:13:35.335209Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  word\n0    ،\n1    ـ\n2    ء\n3   ءَ\n4    آ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>word</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>،</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ـ</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ء</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ءَ</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>آ</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# --- 1) Put all domain dataframes here in one df\nDOMAIN_DFS = {\n    \"AAT\": AAT,\n    \"HTL\": HTL,\n    \"MOV\": MOV,\n    \"PROD\": PROD,\n    \"RES\": RES,\n    \"RES1\": RES1,\n    \"RES2\": RES2,\n}\n\n# --- 2) force the names if your columns are non-standard ---\nTEXT_COL = None   # e.g., \"text\" / \"review\" / \"sentence\" / \"body\" ...\nLABEL_COL = None  # e.g., \"label\" / \"sentiment\" / \"polarity\" ...\n\nCOMMON_TEXT_COLS = [\"text\", \"Text\", \"sentence\", \"Sentence\", \"review\", \"Review\", \"body\", \"Body\"]\nCOMMON_LABEL_COLS = [\"label\", \"Label\", \"sentiment\", \"Sentiment\", \"polarity\", \"Polarity\"]\n\ndef _guess_col(df, candidates):\n    for c in candidates:\n        if c in df.columns:\n            return c\n    return None\n\ndef unify_one(df, domain):\n    global TEXT_COL, LABEL_COL\n    if TEXT_COL is None:\n        TEXT_COL = _guess_col(df, COMMON_TEXT_COLS)\n    if LABEL_COL is None:\n        LABEL_COL = _guess_col(df, COMMON_LABEL_COLS)\n\n    if TEXT_COL is None or LABEL_COL is None:\n        raise ValueError(\n            f\"Could not auto-detect text/label columns for domain {domain}. \"\n            f\"Set TEXT_COL and LABEL_COL manually above.\"\n        )\n\n    out = df[[TEXT_COL, LABEL_COL]].copy()\n    out.columns = [\"text\", \"label\"]\n    out[\"domain\"] = domain\n    return out\n\nall_parts = []\nfor dname, dframe in DOMAIN_DFS.items():\n    all_parts.append(unify_one(dframe, dname))\n\ndata = pd.concat(all_parts, ignore_index=True)\ndata = data.dropna(subset=[\"text\", \"label\"]).reset_index(drop=True)\n\ndata.head(), data.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:35.336592Z","iopub.execute_input":"2026-01-10T12:13:35.336813Z","iopub.status.idle":"2026-01-10T12:13:35.390355Z","shell.execute_reply.started":"2026-01-10T12:13:35.336790Z","shell.execute_reply":"2026-01-10T12:13:35.389791Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(                                                text  label domain\n 0  حمام الكبريت \\r\\nنصحوني بتجربة حمام الكبريت. ي...    1.0    AAT\n 1  قلعة ساحرة \\r\\nمنظر خلاب للمدينة من أعلى القلع...    1.0    AAT\n 2  تبليسي \\r\\nتبليسي جورجيا\\r\\nمن أجمل المدن التي...    1.0    AAT\n 3  جولة على المدينة القديمة تبليسي < شاردن \\r\\nمم...    1.0    AAT\n 4  احلي اجازه لمحبي الطبيعه \\r\\nالمناظر الخلابه ا...    1.0    AAT,\n (45498, 3))"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Make a flat set of stopwords from your uploaded dataframe (any column will work)\nstopword_set = set()\n\nfor col in stopwords.columns:\n    vals = stopwords[col].astype(str).str.strip()\n    stopword_set.update([v for v in vals if v and v != \"nan\"])\n\nlen(stopword_set), list(next(iter(stopword_set)) for _ in range(min(10, len(stopword_set))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:35.392680Z","iopub.execute_input":"2026-01-10T12:13:35.392998Z","iopub.status.idle":"2026-01-10T12:13:35.399884Z","shell.execute_reply.started":"2026-01-10T12:13:35.392979Z","shell.execute_reply":"2026-01-10T12:13:35.399323Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(799,\n ['اللواتي',\n  'اللواتي',\n  'اللواتي',\n  'اللواتي',\n  'اللواتي',\n  'اللواتي',\n  'اللواتي',\n  'اللواتي',\n  'اللواتي',\n  'اللواتي'])"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# --- Normalization helpers (simple + practical) ---\ndef normalize_arabic(text: str) -> str:\n    text = str(text)\n\n    # remove tatweel (kashida) and normalize common hamza/aleph variants\n    text = re.sub(r\"[\\u0640]\", \"\", text)  # tatweel\n    text = araby.normalize_hamza(text)     # normalize hamza forms (PyArabic)\n    text = araby.strip_diacritics(text)    # remove diacritics (tashkeel)\n\n    # basic spacing cleanup\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\ndef simple_tokenize(text: str) -> list:\n    # PyArabic has a tokenizer that returns word tokens (good baseline)\n    try:\n        tokens = list(araby.tokenize(text))\n    except Exception:\n        # fallback: whitespace split (safe)\n        tokens = text.split()\n    return [t for t in tokens if t]\n\ndef remove_stopwords(tokens: list, stop_set: set) -> list:\n    return [t for t in tokens if t not in stop_set and t.strip() != \"\"]\n\ndef preprocess(text: str) -> str:\n    norm = normalize_arabic(text)\n    tokens = simple_tokenize(norm)\n    tokens = remove_stopwords(tokens, stopword_set)\n    return \" \".join(tokens)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:35.400584Z","iopub.execute_input":"2026-01-10T12:13:35.400814Z","iopub.status.idle":"2026-01-10T12:13:35.412982Z","shell.execute_reply.started":"2026-01-10T12:13:35.400789Z","shell.execute_reply":"2026-01-10T12:13:35.412395Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# mapping the labels\n\nlabel_map = {\n    \"positive\": \"positive\",\n    \"pos\": \"positive\",\n    \"neg\": \"negative\",\n    \"negative\": \"negative\",\n    \"neutral\": \"neutral\",\n    \"neu\": \"neutral\",\n    \"0\": \"neutral\",\n    \"1\": \"positive\",\n    \"-1\": \"negative\",\n    0: \"neutral\",\n    1: \"positive\",\n    -1: \"negative\",\n}\n\ndata[\"text_clean\"] = data[\"text\"].astype(str).apply(preprocess)\ndata[\"label_std\"] = data[\"label\"].apply(lambda x: label_map.get(x, str(x).lower()))\n\ndata[[\"text\", \"text_clean\", \"label\", \"label_std\", \"domain\"]].head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:35.413970Z","iopub.execute_input":"2026-01-10T12:13:35.414241Z","iopub.status.idle":"2026-01-10T12:13:42.253846Z","shell.execute_reply.started":"2026-01-10T12:13:35.414218Z","shell.execute_reply":"2026-01-10T12:13:42.252514Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                                                text  \\\n0  حمام الكبريت \\r\\nنصحوني بتجربة حمام الكبريت. ي...   \n1  قلعة ساحرة \\r\\nمنظر خلاب للمدينة من أعلى القلع...   \n2  تبليسي \\r\\nتبليسي جورجيا\\r\\nمن أجمل المدن التي...   \n3  جولة على المدينة القديمة تبليسي < شاردن \\r\\nمم...   \n4  احلي اجازه لمحبي الطبيعه \\r\\nالمناظر الخلابه ا...   \n\n                                          text_clean  label label_std domain  \n0  حمام الكبريت نصحوني بتجربة حمام الكبريت . يمكن...    1.0  positive    AAT  \n1  قلعة ساحرة منظر خلاب للمدينة ءعلى القلعة . يوج...    1.0  positive    AAT  \n2  تبليسي تبليسي جورجيا ءجمل المدن زرتها حياتي شع...    1.0  positive    AAT  \n3  جولة المدينة القديمة تبليسي < شاردن ممتعة الجو...    1.0  positive    AAT  \n4  احلي اجازه لمحبي الطبيعه المناظر الخلابه الطبي...    1.0  positive    AAT  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>label</th>\n      <th>label_std</th>\n      <th>domain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>حمام الكبريت \\r\\nنصحوني بتجربة حمام الكبريت. ي...</td>\n      <td>حمام الكبريت نصحوني بتجربة حمام الكبريت . يمكن...</td>\n      <td>1.0</td>\n      <td>positive</td>\n      <td>AAT</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>قلعة ساحرة \\r\\nمنظر خلاب للمدينة من أعلى القلع...</td>\n      <td>قلعة ساحرة منظر خلاب للمدينة ءعلى القلعة . يوج...</td>\n      <td>1.0</td>\n      <td>positive</td>\n      <td>AAT</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>تبليسي \\r\\nتبليسي جورجيا\\r\\nمن أجمل المدن التي...</td>\n      <td>تبليسي تبليسي جورجيا ءجمل المدن زرتها حياتي شع...</td>\n      <td>1.0</td>\n      <td>positive</td>\n      <td>AAT</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>جولة على المدينة القديمة تبليسي &lt; شاردن \\r\\nمم...</td>\n      <td>جولة المدينة القديمة تبليسي &lt; شاردن ممتعة الجو...</td>\n      <td>1.0</td>\n      <td>positive</td>\n      <td>AAT</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>احلي اجازه لمحبي الطبيعه \\r\\nالمناظر الخلابه ا...</td>\n      <td>احلي اجازه لمحبي الطبيعه المناظر الخلابه الطبي...</td>\n      <td>1.0</td>\n      <td>positive</td>\n      <td>AAT</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_df, temp_df = train_test_split(\n    data, test_size=0.20, stratify=data[\"label_std\"], random_state=42\n)\nval_df, test_df = train_test_split(\n    temp_df, test_size=0.50, stratify=temp_df[\"label_std\"], random_state=42\n)\n\nlen(train_df), len(val_df), len(test_df), train_df[\"label_std\"].value_counts()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:42.254775Z","iopub.execute_input":"2026-01-10T12:13:42.255024Z","iopub.status.idle":"2026-01-10T12:13:42.330692Z","shell.execute_reply.started":"2026-01-10T12:13:42.254997Z","shell.execute_reply":"2026-01-10T12:13:42.330089Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(36398,\n 4550,\n 4550,\n label_std\n positive    26402\n negative     7469\n neutral      2527\n Name: count, dtype: int64)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"outputs/arabic_sentiment\",\n    eval_strategy=\"epoch\",        \n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1_macro\",\n    \n    # Core training parameters\n    do_train=True,\n    do_eval=True,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    num_train_epochs=3,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    logging_steps=50,\n    report_to=\"none\",\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:42.331392Z","iopub.execute_input":"2026-01-10T12:13:42.331659Z","iopub.status.idle":"2026-01-10T12:13:42.368010Z","shell.execute_reply.started":"2026-01-10T12:13:42.331640Z","shell.execute_reply":"2026-01-10T12:13:42.367510Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Recommended compact Arabic encoder for classification (BERT-style):\n# asafaya/bert-base-arabic is a strong, widely used Arabic baseline model. :contentReference[oaicite:2]{index=2}\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nMODEL_NAME = \"asafaya/bert-base-arabic\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3\n)\n\nlabel2id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\nid2label = {v: k for k, v in label2id.items()}\n\ndef encode_batch(df: pd.DataFrame):\n    return tokenizer(\n        df[\"text_clean\"].tolist(),\n        padding=True,\n        truncation=True,\n        max_length=256\n    )\n\nclass PandasDataset(torch.utils.data.Dataset):\n    def __init__(self, df):\n        self.enc = encode_batch(df)\n        self.labels = [label2id[l] for l in df[\"label_std\"].tolist()]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        item = {k: torch.tensor(v[idx]) for k, v in self.enc.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_ds = PandasDataset(train_df)\nval_ds   = PandasDataset(val_df)\ntest_ds  = PandasDataset(test_df)\n\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds = np.argmax(pred.predictions, axis=1)\n    return {\n        \"accuracy\": float(accuracy_score(labels, preds)),\n        \"f1_macro\": float(f1_score(labels, preds, average=\"macro\"))\n    }\n\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:42.368858Z","iopub.execute_input":"2026-01-10T12:13:42.369032Z","iopub.status.idle":"2026-01-10T12:13:55.265814Z","shell.execute_reply.started":"2026-01-10T12:13:42.369018Z","shell.execute_reply":"2026-01-10T12:13:55.265142Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/62.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82b1813cc91341b6935c5add3f3edc81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/491 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afa67454fe864919ae6c0b827c9f84f0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6950a9418b4541a180ee8ec3879bea43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d89e0d248a4a82900ae2125965f91b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/445M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdffaa5c0b6543709a15fcc9c0b3ecac"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at asafaya/bert-base-arabic and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"trainer.train()\n\n# Evaluate on test\ntest_preds = trainer.predict(test_ds)\ny_true = test_preds.label_ids\ny_pred = np.argmax(test_preds.predictions, axis=1)\n\nprint(\"Test accuracy:\", accuracy_score(y_true, y_pred))\nprint(\"Test macro-F1:\", f1_score(y_true, y_pred, average=\"macro\"))\nprint(\"\\nClassification report (test):\")\nprint(classification_report(y_true, y_pred, target_names=[id2label[i] for i in range(3)]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T12:13:55.266615Z","iopub.execute_input":"2026-01-10T12:13:55.266859Z","iopub.status.idle":"2026-01-10T13:09:41.030497Z","shell.execute_reply.started":"2026-01-10T12:13:55.266832Z","shell.execute_reply":"2026-01-10T13:09:41.029752Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6825' max='6825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6825/6825 54:59, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.368700</td>\n      <td>0.308164</td>\n      <td>0.889011</td>\n      <td>0.740654</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.258600</td>\n      <td>0.309657</td>\n      <td>0.894945</td>\n      <td>0.749052</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.115800</td>\n      <td>0.387100</td>\n      <td>0.904396</td>\n      <td>0.772609</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test accuracy: 0.8982417582417582\nTest macro-F1: 0.751030540078813\n\nClassification report (test):\n              precision    recall  f1-score   support\n\n    negative       0.87      0.86      0.86       934\n     neutral       0.48      0.41      0.44       316\n    positive       0.94      0.96      0.95      3300\n\n    accuracy                           0.90      4550\n   macro avg       0.76      0.74      0.75      4550\nweighted avg       0.89      0.90      0.90      4550\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"save_dir = \"models/arabic_sentiment_model\"\ntrainer.save_model(save_dir)\ntokenizer.save_pretrained(save_dir)\n\nprint(\"Saved model + tokenizer to:\", save_dir)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:09:41.031306Z","iopub.execute_input":"2026-01-10T13:09:41.031543Z","iopub.status.idle":"2026-01-10T13:09:41.828133Z","shell.execute_reply.started":"2026-01-10T13:09:41.031521Z","shell.execute_reply":"2026-01-10T13:09:41.827455Z"}},"outputs":[{"name":"stdout","text":"Saved model + tokenizer to: models/arabic_sentiment_model\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\nimport numpy as np\n\ndef predict_with_key_tokens(text, top_k=5, max_words=40):\n    model.eval()\n    device = model.device\n\n    # simple whitespace split\n    words = text.split()\n    if len(words) > max_words:\n        words = words[:max_words]\n\n    # 1) baseline full prediction\n    enc_full = tokenizer(\" \".join(words), return_tensors=\"pt\", truncation=True, max_length=256)\n    enc_full = {k: v.to(device) for k, v in enc_full.items()}   # <<< FIX\n    with torch.no_grad():\n        logits_full = model(**enc_full).logits\n\n    probs_full = torch.softmax(logits_full, dim=-1).cpu().numpy()[0]\n    pred_id = int(np.argmax(probs_full))\n\n    # 2) leave-one-out token importance\n    token_scores = []\n\n    for i, w in enumerate(words):\n        reduced = words[:i] + words[i+1:]\n        if not reduced:  # skip single-token edge\n            continue\n\n        enc = tokenizer(\" \".join(reduced), return_tensors=\"pt\", truncation=True, max_length=256)\n        enc = {k: v.to(device) for k, v in enc.items()}        # <<< FIX\n        with torch.no_grad():\n            logits = model(**enc).logits\n\n        probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n        drop = probs_full[pred_id] - probs[pred_id]\n        token_scores.append((w, float(drop)))\n\n    token_scores = sorted(token_scores, key=lambda x: x[1], reverse=True)\n    key_tokens = [w for w, s in token_scores[:top_k]]\n\n    label = [\"negative\", \"neutral\", \"positive\"][pred_id]\n\n    return {\n        \"label\": label,\n        \"probabilities\": probs_full.tolist(),\n        \"key_words\": key_tokens\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:09:41.828889Z","iopub.execute_input":"2026-01-10T13:09:41.829141Z","iopub.status.idle":"2026-01-10T13:09:41.837019Z","shell.execute_reply.started":"2026-01-10T13:09:41.829117Z","shell.execute_reply":"2026-01-10T13:09:41.836340Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# for 1 example/dummyexample\nexample = \"هذا الفيلم كان ممتعًا جدًا لكن النهاية كانت ضعيفة.\"\npredict_with_key_tokens(example, top_k=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:09:41.837824Z","iopub.execute_input":"2026-01-10T13:09:41.838047Z","iopub.status.idle":"2026-01-10T13:09:41.947680Z","shell.execute_reply.started":"2026-01-10T13:09:41.838031Z","shell.execute_reply":"2026-01-10T13:09:41.946927Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'label': 'positive',\n 'probabilities': [0.2891921401023865, 0.19323374330997467, 0.5175741314888],\n 'key_words': ['ممتعًا', 'لكن', 'كانت', 'ضعيفة.', 'هذا']}"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"test_df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:09:41.948506Z","iopub.execute_input":"2026-01-10T13:09:41.948759Z","iopub.status.idle":"2026-01-10T13:09:41.964022Z","shell.execute_reply.started":"2026-01-10T13:09:41.948734Z","shell.execute_reply":"2026-01-10T13:09:41.962838Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"                                                    text  label domain  \\\n7607   لا انصح \\r\\nلا انصح بهذا الفندق للعائلات من من...   -1.0    HTL   \n42820                       الذ واطعم مطبق ذقته في حياتي    1.0   RES1   \n34907  كان الجو حلو ...جدا في الرياض  \\n  \\n وكنت موا...    1.0   RES1   \n36459  يتميز المطعم بوجود وجبات على المنيو او حسب الر...    1.0   RES1   \n23775  جيد وليس أكثر \\nوجدت المطعم مرشح من قبل العديد...    0.0    RES   \n...                                                  ...    ...    ...   \n23616  ﺣﻜﺎﻳﺔ \\nعلي طول في شارع صغير جنب البانتيون بال...    1.0    RES   \n17755  ذاكرة الجسد أضحى جسداً دون ذاكرة تذكر.. ذاكرة ...   -1.0    MOV   \n36663  جربت عنده الأضلاع 3 مرات وكان ممتازة جداً أعجب...    1.0   RES1   \n37136                 ارز اللوان وين فيه ياليته طعمه زين   -1.0   RES1   \n19065  عندما تتوقع النهاية مع بداية الفيلم The Thing ...    0.0    MOV   \n\n                                              text_clean label_std  \n7607   انصح انصح الفندق للعاءلات منطقة البحر المتوسط ...  negative  \n42820                          الذ واطعم مطبق ذقته حياتي  positive  \n34907  الجو حلو ... الرياض وكنت مواعد العاءلة بطلعه ت...  positive  \n36459  يتميز المطعم بوجود وجبات المنيو الرغبة بالاختي...  positive  \n23775  جيد ءكثر وجدت المطعم مرشح العديد تقارير مواقع ...   neutral  \n...                                                  ...       ...  \n23616  ﺣﻜﺎﻳﺔ طول شارع صغير جنب البانتيون بالصدفة دخلت...  positive  \n17755  ذاكرة الجسد ءضحى جسدا ذاكرة تذكر .. ذاكرة الجس...  negative  \n36663  جربت عنده الءضلاع 3 مرات ممتازة ءعجبتني الاضلا...  positive  \n37136                     ارز اللوان وين ياليته طعمه زين  negative  \n19065  تتوقع النهاية بداية الفيلم The Thing ءو الشيء ...   neutral  \n\n[4550 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n      <th>domain</th>\n      <th>text_clean</th>\n      <th>label_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>7607</th>\n      <td>لا انصح \\r\\nلا انصح بهذا الفندق للعائلات من من...</td>\n      <td>-1.0</td>\n      <td>HTL</td>\n      <td>انصح انصح الفندق للعاءلات منطقة البحر المتوسط ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>42820</th>\n      <td>الذ واطعم مطبق ذقته في حياتي</td>\n      <td>1.0</td>\n      <td>RES1</td>\n      <td>الذ واطعم مطبق ذقته حياتي</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>34907</th>\n      <td>كان الجو حلو ...جدا في الرياض  \\n  \\n وكنت موا...</td>\n      <td>1.0</td>\n      <td>RES1</td>\n      <td>الجو حلو ... الرياض وكنت مواعد العاءلة بطلعه ت...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>36459</th>\n      <td>يتميز المطعم بوجود وجبات على المنيو او حسب الر...</td>\n      <td>1.0</td>\n      <td>RES1</td>\n      <td>يتميز المطعم بوجود وجبات المنيو الرغبة بالاختي...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>23775</th>\n      <td>جيد وليس أكثر \\nوجدت المطعم مرشح من قبل العديد...</td>\n      <td>0.0</td>\n      <td>RES</td>\n      <td>جيد ءكثر وجدت المطعم مرشح العديد تقارير مواقع ...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23616</th>\n      <td>ﺣﻜﺎﻳﺔ \\nعلي طول في شارع صغير جنب البانتيون بال...</td>\n      <td>1.0</td>\n      <td>RES</td>\n      <td>ﺣﻜﺎﻳﺔ طول شارع صغير جنب البانتيون بالصدفة دخلت...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>17755</th>\n      <td>ذاكرة الجسد أضحى جسداً دون ذاكرة تذكر.. ذاكرة ...</td>\n      <td>-1.0</td>\n      <td>MOV</td>\n      <td>ذاكرة الجسد ءضحى جسدا ذاكرة تذكر .. ذاكرة الجس...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>36663</th>\n      <td>جربت عنده الأضلاع 3 مرات وكان ممتازة جداً أعجب...</td>\n      <td>1.0</td>\n      <td>RES1</td>\n      <td>جربت عنده الءضلاع 3 مرات ممتازة ءعجبتني الاضلا...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>37136</th>\n      <td>ارز اللوان وين فيه ياليته طعمه زين</td>\n      <td>-1.0</td>\n      <td>RES1</td>\n      <td>ارز اللوان وين ياليته طعمه زين</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>19065</th>\n      <td>عندما تتوقع النهاية مع بداية الفيلم The Thing ...</td>\n      <td>0.0</td>\n      <td>MOV</td>\n      <td>تتوقع النهاية بداية الفيلم The Thing ءو الشيء ...</td>\n      <td>neutral</td>\n    </tr>\n  </tbody>\n</table>\n<p>4550 rows × 5 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# for all test data\npredict_with_key_tokens(test_df.iloc[0][\"text_clean\"], top_k=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:09:41.964621Z","iopub.execute_input":"2026-01-10T13:09:41.964879Z","iopub.status.idle":"2026-01-10T13:09:42.339568Z","shell.execute_reply.started":"2026-01-10T13:09:41.964856Z","shell.execute_reply":"2026-01-10T13:09:42.338832Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'label': 'positive',\n 'probabilities': [0.032135214656591415,\n  0.14049628376960754,\n  0.8273684978485107],\n 'key_words': ['الءخضر', 'ولونه', 'للعاءلات', 'العشب', 'وخلافة']}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"results = []\n\nfor idx, row in test_df.iterrows():\n    output = predict_with_key_tokens(row[\"text_clean\"], top_k=5)\n\n    results.append({\n        \"index\": idx,\n        \"text_clean\": row[\"text_clean\"],\n        \"predicted_label\": output[\"label\"],\n        \"prob_class_0\": output[\"probabilities\"][0],\n        \"prob_class_1\": output[\"probabilities\"][1],\n        \"prob_class_2\": output[\"probabilities\"][2],\n        \"key_words\": output[\"key_words\"]\n    })\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:09:42.340373Z","iopub.execute_input":"2026-01-10T13:09:42.340713Z","iopub.status.idle":"2026-01-10T13:27:03.244100Z","shell.execute_reply.started":"2026-01-10T13:09:42.340686Z","shell.execute_reply":"2026-01-10T13:27:03.243357Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"results_df = pd.DataFrame(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:27:03.244994Z","iopub.execute_input":"2026-01-10T13:27:03.245327Z","iopub.status.idle":"2026-01-10T13:27:03.258808Z","shell.execute_reply.started":"2026-01-10T13:27:03.245307Z","shell.execute_reply":"2026-01-10T13:27:03.258109Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"results_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:27:03.259473Z","iopub.execute_input":"2026-01-10T13:27:03.259667Z","iopub.status.idle":"2026-01-10T13:27:03.277941Z","shell.execute_reply.started":"2026-01-10T13:27:03.259646Z","shell.execute_reply":"2026-01-10T13:27:03.277366Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"   index                                         text_clean predicted_label  \\\n0   7607  انصح انصح الفندق للعاءلات منطقة البحر المتوسط ...        positive   \n1  42820                          الذ واطعم مطبق ذقته حياتي        positive   \n2  34907  الجو حلو ... الرياض وكنت مواعد العاءلة بطلعه ت...        positive   \n3  36459  يتميز المطعم بوجود وجبات المنيو الرغبة بالاختي...        positive   \n4  23775  جيد ءكثر وجدت المطعم مرشح العديد تقارير مواقع ...         neutral   \n\n   prob_class_0  prob_class_1  prob_class_2  \\\n0      0.032135      0.140496      0.827368   \n1      0.001119      0.000324      0.998556   \n2      0.001001      0.000586      0.998413   \n3      0.000763      0.000636      0.998601   \n4      0.002502      0.988225      0.009273   \n\n                                  key_words  \n0  [الءخضر, ولونه, للعاءلات, العشب, وخلافة]  \n1           [واطعم, الذ, ذقته, حياتي, مطبق]  \n2    [حلو, مقبلات, تشكيلة, والانتظار, رحنا]  \n3      [راءع, شي, السلطة, بوفيه, بالاختيار]  \n4           [جيد, ءكثر, جيد, مرشح, بالمميز]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>text_clean</th>\n      <th>predicted_label</th>\n      <th>prob_class_0</th>\n      <th>prob_class_1</th>\n      <th>prob_class_2</th>\n      <th>key_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7607</td>\n      <td>انصح انصح الفندق للعاءلات منطقة البحر المتوسط ...</td>\n      <td>positive</td>\n      <td>0.032135</td>\n      <td>0.140496</td>\n      <td>0.827368</td>\n      <td>[الءخضر, ولونه, للعاءلات, العشب, وخلافة]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>42820</td>\n      <td>الذ واطعم مطبق ذقته حياتي</td>\n      <td>positive</td>\n      <td>0.001119</td>\n      <td>0.000324</td>\n      <td>0.998556</td>\n      <td>[واطعم, الذ, ذقته, حياتي, مطبق]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34907</td>\n      <td>الجو حلو ... الرياض وكنت مواعد العاءلة بطلعه ت...</td>\n      <td>positive</td>\n      <td>0.001001</td>\n      <td>0.000586</td>\n      <td>0.998413</td>\n      <td>[حلو, مقبلات, تشكيلة, والانتظار, رحنا]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>36459</td>\n      <td>يتميز المطعم بوجود وجبات المنيو الرغبة بالاختي...</td>\n      <td>positive</td>\n      <td>0.000763</td>\n      <td>0.000636</td>\n      <td>0.998601</td>\n      <td>[راءع, شي, السلطة, بوفيه, بالاختيار]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>23775</td>\n      <td>جيد ءكثر وجدت المطعم مرشح العديد تقارير مواقع ...</td>\n      <td>neutral</td>\n      <td>0.002502</td>\n      <td>0.988225</td>\n      <td>0.009273</td>\n      <td>[جيد, ءكثر, جيد, مرشح, بالمميز]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# evaluating the model\n\ndef eval_by_domain(df, name=\"domain\"):\n    rows = []\n    for dom, part in df.groupby(name):\n        ds = PandasDataset(part)\n        pred = trainer.predict(ds)\n        y_true = pred.label_ids\n        y_pred = np.argmax(pred.predictions, axis=1)\n        rows.append({\n            \"domain\": dom,\n            \"n\": len(part),\n            \"accuracy\": accuracy_score(y_true, y_pred),\n            \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n        })\n    return pd.DataFrame(rows).sort_values(\"n\", ascending=False)\n\neval_by_domain(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-10T13:27:22.178727Z","iopub.execute_input":"2026-01-10T13:27:22.179284Z","iopub.status.idle":"2026-01-10T13:28:06.235232Z","shell.execute_reply.started":"2026-01-10T13:27:22.179262Z","shell.execute_reply":"2026-01-10T13:28:06.234520Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"  domain     n  accuracy  f1_macro\n1    HTL  1590  0.843396  0.716536\n4    RES  1113  0.957772  0.869915\n5   RES1   826  0.970944  0.644831\n3   PROD   400  0.807500  0.555030\n6   RES2   270  0.948148  0.896297\n0    AAT   208  0.966346  0.550505\n2    MOV   143  0.685315  0.562727","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>domain</th>\n      <th>n</th>\n      <th>accuracy</th>\n      <th>f1_macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>HTL</td>\n      <td>1590</td>\n      <td>0.843396</td>\n      <td>0.716536</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RES</td>\n      <td>1113</td>\n      <td>0.957772</td>\n      <td>0.869915</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>RES1</td>\n      <td>826</td>\n      <td>0.970944</td>\n      <td>0.644831</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>PROD</td>\n      <td>400</td>\n      <td>0.807500</td>\n      <td>0.555030</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>RES2</td>\n      <td>270</td>\n      <td>0.948148</td>\n      <td>0.896297</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>AAT</td>\n      <td>208</td>\n      <td>0.966346</td>\n      <td>0.550505</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MOV</td>\n      <td>143</td>\n      <td>0.685315</td>\n      <td>0.562727</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29}]}